{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Analytics using Python\n",
    "\n",
    "*This is the exercise notebook which will help you practice the steps followed to solve a credit card fraud problem. You will learn how to make use of several Machine learning algorithms to solve financial crime problem. Later we'll try to optimise the solution wherever needed using pre-built methods.*\n",
    "\n",
    "The OLT covers the following topics:\n",
    "\n",
    "* Introduction to fraud detection\n",
    "    * Reading the data labels\n",
    "    * Data resampling and plotting\n",
    "    * Applying SMOTE\n",
    "    * Logistic regression with SMOTE \n",
    "* Using ML classification to catch fraud\n",
    "    * Random forest classifier\n",
    "    * Performance of RF classifier\n",
    "    * Plotting precision recall curve  \n",
    "* Performing model adjustments and regression analysis\n",
    "    * GridSearchCV to find optimal parameters\n",
    "    * Logistic regression\n",
    "    * Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the occurrences of fraud and no fraud and print them\n",
    "# Import pandas and read csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/creditcard.csv\")\n",
    "\n",
    "# Explore the features available in your dataframe\n",
    "print(df._____)\n",
    "\n",
    "# Count the occurrences of fraud and no fraud and print them\n",
    "occ = df['____'].value_counts()\n",
    "print(occ)\n",
    "\n",
    "# Print the ratio of fraud cases\n",
    "print(occ / ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data \n",
    "\n",
    "In this exercise, you'll look at the data and visualize the fraud to non-fraud ratio. It is always a good starting point in your fraud analysis, to look at your data first, before you make any changes to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "count_classes = pd.value_counts(df['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    \"\"\"\n",
    "    Prepare the data to train the model\n",
    "    args: df - dataframe\n",
    "    returns: X - array of columns\n",
    "    y - class array to be predicted\n",
    "    \"\"\"\n",
    "    # accumulate feature variables in X and target variable in y using slicing\n",
    "    # <----code goes here---->\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the cell below:\n",
    "\n",
    "* Define the plot_data(X, y) function, that will nicely plot the given feature set X with labels y in a scatter plot. This has been done for you.\n",
    "\n",
    "* Use the function prep_data() on your dataset df to create feature set X and labels y.\n",
    "\n",
    "* Run the function plot_data() on your newly obtained X and y to visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a scatter plot of our data and labels\n",
    "def plot_data(X, y):\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "# Create X and y from the prep_data function \n",
    "X, y = prep_data(____)\n",
    "\n",
    "# Plot our data by running our plot data function on X and y\n",
    "____(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to manage imbalanced data?\n",
    "\n",
    "Let's learn different methods which can help us deal with imbalanced data.\n",
    "There are various resmapling techniques to handle imbalanced data:\n",
    "\n",
    "* `Random Under Sampling(RUS)`: This reduces the majority class and makes the data balanced.\n",
    "* `Random Over Sampling(ROS)`: This generated duplicates of the minority class. Inefficient because of duplicacy.\n",
    "* `Synthetic Minority Oversampling Technique(SMOTE)`: Generates fake realistic data to balance out the data.\n",
    "\n",
    "We are going to use the best possible option which is SMOTE here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Run the prep_data function\n",
    "X, y = ____(df)\n",
    "\n",
    "# Define the resampling method\n",
    "method = ____(kind='____')\n",
    "\n",
    "# Create the resampled feature set\n",
    "X_resampled, y_resampled = method.____(____, ____)\n",
    "\n",
    "# Plot the resampled data\n",
    "plot_data(____, ____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(X,y,X_resampled,y_resampled, method):\n",
    "    # Start a plot figure\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    \n",
    "    # sub-plot number 1, this is our normal data\n",
    "    c0 = ax1.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\",alpha=0.5)\n",
    "    c1 = ax1.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\",alpha=0.5, c='r')\n",
    "    ax1.set_title('Original set')\n",
    "    \n",
    "    # sub-plot number 2, this is our oversampled data\n",
    "    ax2.scatter(X_resampled[y_resampled == 0, 0], X_resampled[y_resampled == 0, 1], label=\"Class #0\", alpha=.5)\n",
    "    ax2.scatter(X_resampled[y_resampled == 1, 0], X_resampled[y_resampled == 1, 1], label=\"Class #1\", alpha=.5,c='r')\n",
    "    ax2.set_title(method)\n",
    "    \n",
    "    # some settings and ready to go\n",
    "    plt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n",
    "                  ncol=2, labelspacing=0.)\n",
    "    plt.tight_layout(pad=3)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the cell below:\n",
    "\n",
    "* Print the value counts of our original labels, y. Be mindful that y is currently a Numpy array, so in order to use value counts, we'll assign y back as a pandas Series object.\n",
    "* Repeat the step and print the value counts on y_resampled. This shows you how the balance between the two classes has changed with SMOTE.\n",
    "* Use the compare_plot() function called on our original data as well our resampled data to see the scatterplots side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value_counts on the original labels y\n",
    "print(pd.value_counts(pd.Series(____)))\n",
    "\n",
    "# Print the value_counts\n",
    "print(____(____(____)))\n",
    "\n",
    "# Run compare_plot\n",
    "compare_plot(____, ____, ____, ____, method='SMOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based method to detect fraudsters exercise\n",
    "\n",
    "In this exercise you're going to try finding fraud cases in our credit card dataset the \"old way\". First you'll define threshold values using common statistics, to split fraud and non-fraud. Then, use those thresholds on your features to detect fraud. This is common practice within fraud analytics teams.\n",
    "\n",
    "Statistical thresholds are often determined by looking at the mean values of observations. Let's start this exercise by checking whether feature means differ between fraud and non-fraud cases. Then, you'll use that information to create common sense thresholds. Finally, you'll check how well this performs in fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean for each group\n",
    "____.____(____).mean()\n",
    "\n",
    "# Implement a rule for stating which cases are flagged as fraud\n",
    "df['flag_as_fraud'] = np.where(np.logical_and(______), 1, 0)\n",
    "\n",
    "# Create a crosstab of flagged fraud cases versus the actual fraud cases\n",
    "print(____(df.Class, df.flag_as_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Now, using ML classification to catch fraudsters\n",
    "\n",
    "In this exercise you'll see what happens when you use a simple machine learning model on our credit card data instead.\n",
    "\n",
    "Do you think you can beat those results? Remember, you've predicted 170 out of 492 fraud cases, and had 1226 false positives. That's less than half of the cases caught, Also false positives were roughly 3 times the actual amount of fraud cases.\n",
    "\n",
    "So with that in mind, let's implement a Logistic Regression model.[Poll] If not, you might want to refresh that at this point. But don't worry, you'll be guided through the structure of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sklearn for training splitting and importing the classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise below:\n",
    "\n",
    "* Split X and y into training and test data, keeping 30% of the data for testing.\n",
    "* Fit your model to your training data.\n",
    "* Obtain the model predicted labels by running model.predict on X_test.\n",
    "* Obtain a classification comparing y_test with predicted, and use the given confusion matrix to check your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(____, ____, test_size=____, random_state=0)\n",
    "\n",
    "# Fit a logistic regression model to our data\n",
    "model = LogisticRegression()\n",
    "model.fit(____, ____)\n",
    "\n",
    "# Obtain model predictions\n",
    "predicted = model.predict(____)\n",
    "\n",
    "# Print the classifcation report and confusion matrix\n",
    "print('Classification report:\\n', classification_report(____, ____))\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression combined with SMOTE\n",
    "\n",
    "In this exercise, we're going to take the Logistic Regression model from the previous exercise, and combine that with a SMOTE resampling method. We'll see how to do that efficiently by using a pipeline that combines the resampling method with the model in one go. First, we need to define the pipeline that we're going to use.\n",
    "\n",
    "### Exercise below:\n",
    "\n",
    "* Import the Pipeline module from imblearn, this has been done for you.\n",
    "* Then define what you want to put into the pipeline, assign the SMOTE method with borderline2 to resampling, and assign LogisticRegression() to the model.\n",
    "* The Pipeline() requires two arguments. You need to state you want to combine resampling with the model in the respective arguments, we show you how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pipeline module we need for this from imblearn\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# Define which resampling method and which ML model to use in the pipeline\n",
    "resampling = ____\n",
    "model = ____\n",
    "\n",
    "# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\n",
    "pipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have our pipeline defined which is a combination of logistic regression with a SMOTE method, let's run it on the data. You can treat the pipeline as if it were a single machine learning model. Our data X and y are already defined, and the pipeline is defined in the previous exercise. Are you curious to find out what the model results are? Let's give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise on using a pipeline below:\n",
    "\n",
    "* Split the data 'X'and 'y' into the training and test set. Set aside 30% of the data for a test set, and set the random_state to zero.\n",
    "* Fit your pipeline onto your training data and obtain the predictions by running the pipeline.predict() function on our X_test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data X and y, into a training and a test set and fit the pipeline onto the training data\n",
    "X_train, X_test, y_train, y_test = ____\n",
    "\n",
    "# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data \n",
    "pipeline.fit(____, ____) \n",
    "predicted = pipeline.____(____)\n",
    "\n",
    "# Obtain the results from the classification report and confusion matrix \n",
    "print('Classifcation report:\\n', classification_report(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! As you can see, the SMOTE hasn't helped but has added more false positives in our results. We have a very high number of false positives. Remember, not in all cases does resampling necessarily lead to better results. When the fraud cases are very spread and scattered over the data, using SMOTE can introduce a bit of bias. Nearest neighbors aren't necessarily also fraud cases, so the synthetic samples might 'confuse' the model slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud detection using labelled data\n",
    "\n",
    "Now that you're familiar with the main challenges of fraud detection, you're about to learn how to flag fraudulent transactions with supervised learning. You will use classifiers, adjust them and compare them to find the most efficient fraud detection model.\n",
    "\n",
    "### Natural hit rate\n",
    "\n",
    "* Count the total number of observations by taking the length of your labels `y`.\n",
    "* Count the non-fraud cases in our data by using list comprehension on `y`; remember `y` is a NumPy array so `.value_counts()` cannot be used in this case.\n",
    "* Calculate the natural accuracy by dividing the non-fraud cases over the total observations.\n",
    "Print the percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of observations from the length of y\n",
    "total_obs = ____\n",
    "\n",
    "# Count the total number of non-fraudulent observations \n",
    "non_fraud = [i for ____ ____ ____ if i == 0]\n",
    "count_non_fraud = non_fraud.count(0)\n",
    "\n",
    "# Calculate the percentage of non fraud observations in the dataset\n",
    "percentage = (float(____)/float(____)) * 100\n",
    "\n",
    "# Print the percentage: this is our \"natural accuracy\" by doing nothing\n",
    "____(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.bincount(y.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Random forest classifier \n",
    "\n",
    "* Import the random forest classifier from `sklearn`.\n",
    "* Split your features `X` and labels `y` into a training and test set. Set aside a test set of 30%.\n",
    "* Assign the random forest classifier to `model` and keep `random_state` at 5. We need to set a random state here in order to be able to compare results across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the random forest model from sklearn\n",
    "from sklearn.ensemble import ____\n",
    "\n",
    "# Split your data into training and test set\n",
    "X_train, X_test, y_train, y_test = ____(____, ____, test_size=____, random_state=0)\n",
    "\n",
    "# Define the model as the random forest\n",
    "model = ____(random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain predictions from the test data \n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Print the accuracy performance metric\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics for the RF model\n",
    "\n",
    "With highly imbalanced fraud data, the AUROC curve is a more reliable performance metric, used to compare different classifiers. Moreover, the classification report tells you about the precision and recall of your model, whilst the confusion matrix actually shows how many fraud cases you can predict correctly\n",
    "\n",
    "### Exercise below:\n",
    "\n",
    "* Import the classification report, confusion matrix and ROC score from sklearn.metrics.\n",
    "* Get the binary predictions from your trained random forest model.\n",
    "* Get the predicted probabilities by running the predict_proba() function.\n",
    "* Obtain classification report and confusion matrix by comparing y_test with predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages to get the different performance metrics\n",
    "from sklearn.metrics import ____, ____, ____\n",
    "\n",
    "# Obtain the predictions from our random forest model \n",
    "predicted = model.____(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = ____.____(X_test)\n",
    "\n",
    "# Print the ROC curve, classification report and confusion matrix\n",
    "print(____(y_test, probs[:,1]))\n",
    "print(____(____, predicted))\n",
    "print(____(____, ____))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Precision Recall Curve\n",
    "In this curve Precision and Recall are inversely related; as Precision increases, Recall falls and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(recall, precision, average_precision):\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_curve\n",
    "\n",
    "# Calculate average precision and the PR curve\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "\n",
    "# Obtain precision and recall \n",
    "precision, recall, _ = precision_recall_curve(y_test, predicted)\n",
    "\n",
    "# Plot the recall precision tradeoff\n",
    "plot_pr_curve(recall, precision, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve plots the true positives vs. false positives , for a classifier, as its discrimination threshold is varied. Since, a random method describes a horizontal curve through the unit interval, it has an AUC of 0.5. Minimally, classifiers should perform better than this, and the extent to which they score higher than one another (meaning the area under the ROC curve is larger), they have better expected performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create true and false positive rates\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model adjustment exercise below:\n",
    "\n",
    "* Set the class_weight argument of your classifier to balanced_subsample.\n",
    "* Fit your model to your training set.\n",
    "* Obtain predictions and probabilities from X_test.\n",
    "* Obtain the roc_auc_score, the classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with balanced subsample\n",
    "model = RandomForestClassifier(class_weight='____', random_state=5)\n",
    "\n",
    "# Fit your training model to your training set\n",
    "model.fit(____, ____)\n",
    "\n",
    "# Obtain the predicted values and probabilities from the model \n",
    "predicted = ____.____(____)\n",
    "probs = ____.____(____)\n",
    "\n",
    "# Print the roc_auc_score, the classification report and confusion matrix\n",
    "print(____(____, ____))\n",
    "print(____(____, ____))\n",
    "print(____(____, ____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the model results don't improve drastically. If we mostly care about catching fraud, and not so much about the false positives, this does actually not improve our model at all, albeit a simple option to try.\n",
    "\n",
    "## Adjusting the RM to fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    print (classification_report(y_test, predicted))\n",
    "    print (confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the exercise below:\n",
    "\n",
    "* Change the weight option to set the ratio to 1 to 12 for the non-fraud and fraud cases, and set the split criterion to 'entropy'.\n",
    "* Set the maximum depth to 10.\n",
    "* Set the minimal samples in leaf nodes to 10.\n",
    "* Set the number of trees to use in the model to 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model options\n",
    "model = RandomForestClassifier(bootstrap=True, class_weight={0:____, 1:____}, criterion='____',\n",
    "\n",
    "        # Change depth of model\n",
    "        max_depth=____,\n",
    "\n",
    "        # Change the number of samples in leaf nodes\n",
    "        min_samples_leaf=____, \n",
    "\n",
    "        # Change the number of trees to use\n",
    "        n_estimators=____, n_jobs=-1, random_state=5)\n",
    "\n",
    "# Run the function get_model_results\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see by smartly defining more options in the model, you can obtain better predictions. You have effectively reduced the number of false negatives, i.e. you are catching more cases of fraud, whilst keeping the number of false positives low. In this exercise you've manually changed the options of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV to find optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the exercise below:\n",
    "\n",
    "* Define in the parameter grid that you want to try 1 and 30 trees, and that you want to try the gini and entropy split criterion.\n",
    "* Define the model to be simple RandomForestClassifier, you want to keep the random_state at 5 to be able to compare models.\n",
    "* Set the scoring option such that it optimizes for recall.\n",
    "* Fit the model to the training data X_train and y_train and obtain the best parameters for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter sets to test\n",
    "param_grid = {'n_estimators': [____, ____], 'max_features': ['auto', 'log2'],  'max_depth': [4, 8], 'criterion': ['____', '____']\n",
    "}\n",
    "\n",
    "# Define the model to use\n",
    "model = ____(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='____', n_jobs=-1)\n",
    "\n",
    "# Fit the model to our training data and obtain best parameters\n",
    "CV_model.fit(____, ____)\n",
    "CV_model.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the optimal parameters in the model\n",
    "model = RandomForestClassifier(class_weight={0:1,1:12}, ###################################\n",
    "                               criterion='gini',\n",
    "                               n_estimators=30, \n",
    "                               max_features='log2',  \n",
    "                               min_samples_leaf=10, \n",
    "                               max_depth=8,\n",
    "                               n_jobs=-1, random_state=5)\n",
    "\n",
    "# Get results from your model\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've managed to improve your model even further. The number of false positives has now been slightly reduced even further, which means we are catching more cases of fraud. However, you see that the number of false negatives is still the same. That is that Precision-Recall trade-off in action. To decide which final model is best, you need to take into account how bad it is not to catch fraudsters, versus how many false positives the fraud analytics team can deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Exercise below:\n",
    "\n",
    "* Define a LogisticRegression model with class weights that are 1:15 for the fraud cases.\n",
    "* Fit the model to the training set, and obtain the model predictions.\n",
    "* Print the classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model with weights\n",
    "model = ____(____={____, ____}, random_state=5)\n",
    "\n",
    "# Get the model results\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the Logistic Regression has quite different performance from the Random Forest. More false positives, but also a better Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Import the Voting Classifier package.\n",
    "* Define the three models; use the Logistic Regression from before, the Random Forest from previous exercises and a Decision tree with balanced class weights.\n",
    "* Define the ensemble model by inputting the three classifiers with their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "from sklearn.ensemble import ____\n",
    "\n",
    "# Define the three classifiers to use in the ensemble\n",
    "clf1 = LogisticRegression(class_weight={0:1, 1:15}, random_state=5)\n",
    "clf2 = ____(class_weight={0:1, 1:12}, criterion='gini', max_depth=8, max_features='log2',\n",
    "            min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5)\n",
    "clf3 = DecisionTreeClassifier(random_state=5, class_weight=\"____\")\n",
    "\n",
    "# Combine the classifiers in the ensemble model\n",
    "ensemble_model = ____(estimators=[('lr', ____), ('rf', ____), ('dt', ____)], voting='hard')\n",
    "\n",
    "# Get the results \n",
    "get_model_results(X_train, y_train, X_test, y_test, ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust weights within the Voting Classifier\n",
    "\n",
    "* Define an ensemble method where you over weigh the second classifier (clf2) with 4 to 1 to the rest of the classifiers.\n",
    "* Fit the model to the training and test set, and obtain the predictions predicted from the ensemble model.\n",
    "* Print the performance metrics, this is ready for you to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ensemble model\n",
    "ensemble_model = VotingClassifier(  ###################################\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], \n",
    "    voting='soft', \n",
    "    weights=[1, 4, 1], \n",
    "    flatten_transform=True)\n",
    "\n",
    "# Get results \n",
    "get_model_results(X_train, y_train, X_test, y_test, ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the performance metrics and we are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
